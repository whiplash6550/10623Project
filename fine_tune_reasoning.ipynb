{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/10623Project"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WiN8KffXz1Oo","executionInfo":{"status":"ok","timestamp":1744732758933,"user_tz":240,"elapsed":26505,"user":{"displayName":"Wenyuan Shen","userId":"00680151474711071422"}},"outputId":"0dce532b-b158-4755-eb87-8292da3903b2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/10623Project\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E66pBK4iztWB","executionInfo":{"status":"ok","timestamp":1744732858573,"user_tz":240,"elapsed":96298,"user":{"displayName":"Wenyuan Shen","userId":"00680151474711071422"}},"outputId":"0b6395da-bc45-4c9c-e90a-aab31916f4bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bitsandbytes\n","  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n","Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Collecting datasets\n","  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.2)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}],"source":["# !pip install bitsandbytes datasets\n","!pip install -U bitsandbytes\n","!pip install datasets\n","import torch\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorForLanguageModeling\n",")\n","from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n","import os\n","import json"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7w6_M3SvztWC","executionInfo":{"status":"ok","timestamp":1744733027909,"user_tz":240,"elapsed":2118,"user":{"displayName":"Wenyuan Shen","userId":"00680151474711071422"}},"outputId":"0a9f1aed-37c1-46e2-d23e-3c14332153e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'prompt': '### Instruction:\\nSolve the following math problem step by step.\\n\\n### Input:\\nNatalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\\n\\n### Response:', 'completion': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"]}],"source":["# Step 1: Load and process dataset (assuming JSONL format)\n","def load_jsonl_dataset(file_path):\n","    with open(file_path, \"r\") as f:\n","        lines = [json.loads(line) for line in f]\n","    return lines\n","\n","def format_for_instruction(example):\n","    instruction = \"Solve the following math problem step by step.\"\n","    question = example[\"prompt\"].replace(\"Q:\", \"\").replace(\"A:\", \"\").strip()\n","    return {\n","        \"prompt\": f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{question}\\n\\n### Response:\",\n","        \"completion\": example[\"completion\"]\n","    }\n","\n","\n","# Load raw and format\n","data_path = \"gsm8k_reasoning_train.jsonl\"\n","raw_data = load_jsonl_dataset(data_path)\n","formatted_data = [format_for_instruction(ex) for ex in raw_data]\n","print(formatted_data[0])"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["1dca407a31f94b53b8c5518ebc24da58","92e5d87243a34497a8900f080edbedae","f3dd4e6f6c7c43aea5607563c02a8729","7b9e9e5da2b549248b606761400faa7c","12b24569aff1446dbf4584cc7f5dd2ee","080448f4f4db4f69b44e5b7a73f751a2","8ee7ffdfbfba4076b67a479ab5614da0","5bb4177686074528b58854cce7db98de","3e93eda585754a89b42c00927578c29c","616dc3230faf4959a75db4ce09284475","7cb259562dab41d492f2605ebe722b34"]},"id":"ultm9rjoztWC","executionInfo":{"status":"ok","timestamp":1744733714933,"user_tz":240,"elapsed":7711,"user":{"displayName":"Wenyuan Shen","userId":"00680151474711071422"}},"outputId":"0d02624b-4591-42d9-f06a-4ee5bfeabda5"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dca407a31f94b53b8c5518ebc24da58"}},"metadata":{}}],"source":["\n","# Convert to HF dataset\n","# !pip install datasets\n","from datasets import Dataset\n","train_dataset = Dataset.from_list(formatted_data)\n","\n","# Tokenization\n","model_name = \"google/gemma-7b\"\n","hf_token = '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'\n","tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n","\n","def tokenize(example):\n","    full_input = example[\"prompt\"] + example[\"completion\"]\n","    return tokenizer(\n","        full_input,\n","        truncation=True,\n","        max_length=384,\n","        padding=\"max_length\"\n","    )\n","\n","tokenized_dataset = train_dataset.map(tokenize, batched=False)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["9afe9e51249e41c7a311ef8d325380fb","4162d6278a3f48428b613d83651a4e56","6ec521561cc343eeb40cade33be23c02","68a0d27da022430b8885c031c228f13f","6385e3b2293b4646803f17d2469f881d","2f85010846ad4a83aa6be214934789b0","9bc11577a21a474197804cd65223439c","8ffe960af13842d08f124a5cf43db68e","5957a9a92eb64bd79a02bf95f5946ddc","c7c44427d5c2444881c5a8424a49501a","ab8166aeb603412e8702c29b38545629"]},"id":"NZjratm9ztWD","executionInfo":{"status":"ok","timestamp":1744733741072,"user_tz":240,"elapsed":21164,"user":{"displayName":"Wenyuan Shen","userId":"00680151474711071422"}},"outputId":"7a1e1b1f-0337-4302-8ad1-5d91f0a93338"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9afe9e51249e41c7a311ef8d325380fb"}},"metadata":{}}],"source":["\n","# Step 2: Load 4-bit quantized model with BitsAndBytes\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    bnb_4bit_quant_type=\"nf4\"\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    device_map=\"auto\",\n","    quantization_config=bnb_config,\n","    torch_dtype=torch.bfloat16,\n","    token=hf_token\n",")\n","\n","# Prepare for QLoRA\n","model = prepare_model_for_kbit_training(model)\n","\n","# Step 3: Apply LoRA (QLoRA = LoRA on quantized model)\n","lora_config = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Check for Gemma-specific names if needed\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=TaskType.CAUSAL_LM\n",")\n","\n","model = get_peft_model(model, lora_config)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"swkRnK55ztWD","executionInfo":{"status":"ok","timestamp":1744739074455,"user_tz":240,"elapsed":5333375,"user":{"displayName":"Wenyuan Shen","userId":"00680151474711071422"}},"outputId":"f4c17442-5364-4764-fd9c-7f609571bf8c"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-dedb44a7d976>:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='699' max='699' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [699/699 1:28:43, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.718300</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.432400</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.306300</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.166900</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.029400</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.939100</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.887300</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.866300</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.852800</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.847100</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.850800</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.844500</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.845200</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.837500</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.816800</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.849900</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.817500</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.815100</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.828500</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.834400</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.835500</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.806100</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.795700</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.894900</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.807800</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.830600</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.800600</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.803800</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.824100</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.815600</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.803000</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>0.803000</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>0.796400</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>0.797000</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.821800</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.797000</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>0.808800</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>0.798600</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>0.791200</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.809900</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>0.789400</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>0.810700</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>0.799500</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>0.799500</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.806800</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>0.788900</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>0.870600</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>0.810800</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>0.810800</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.800800</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>0.803700</td>\n","    </tr>\n","    <tr>\n","      <td>520</td>\n","      <td>0.795000</td>\n","    </tr>\n","    <tr>\n","      <td>530</td>\n","      <td>0.815700</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>0.774500</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>0.792600</td>\n","    </tr>\n","    <tr>\n","      <td>560</td>\n","      <td>0.796500</td>\n","    </tr>\n","    <tr>\n","      <td>570</td>\n","      <td>0.774600</td>\n","    </tr>\n","    <tr>\n","      <td>580</td>\n","      <td>0.791200</td>\n","    </tr>\n","    <tr>\n","      <td>590</td>\n","      <td>0.804900</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.781200</td>\n","    </tr>\n","    <tr>\n","      <td>610</td>\n","      <td>0.795400</td>\n","    </tr>\n","    <tr>\n","      <td>620</td>\n","      <td>0.789700</td>\n","    </tr>\n","    <tr>\n","      <td>630</td>\n","      <td>0.779300</td>\n","    </tr>\n","    <tr>\n","      <td>640</td>\n","      <td>0.796600</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>0.774800</td>\n","    </tr>\n","    <tr>\n","      <td>660</td>\n","      <td>0.764600</td>\n","    </tr>\n","    <tr>\n","      <td>670</td>\n","      <td>0.797500</td>\n","    </tr>\n","    <tr>\n","      <td>680</td>\n","      <td>0.785700</td>\n","    </tr>\n","    <tr>\n","      <td>690</td>\n","      <td>0.771300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67fe8c22-0b30beea13b3fb8138fa902c;e110d91b-d6ed-4418-997c-007efc0e6421)\n","\n","Cannot access gated repo for url https://huggingface.co/google/gemma-7b/resolve/main/config.json.\n","Access to model google/gemma-7b is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-7b.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in google/gemma-7b - will assume that the vocabulary was not modified.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67fe9218-04c04247176de8e931e923d2;4fda67a9-16e2-4011-8e3e-9cc832236a16)\n","\n","Cannot access gated repo for url https://huggingface.co/google/gemma-7b/resolve/main/config.json.\n","Access to model google/gemma-7b is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-7b.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in google/gemma-7b - will assume that the vocabulary was not modified.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67fe980d-2ba457a321580b0707a30f66;a6b7c771-0043-408d-9c5f-51fa14f39cfa)\n","\n","Cannot access gated repo for url https://huggingface.co/google/gemma-7b/resolve/main/config.json.\n","Access to model google/gemma-7b is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-7b.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in google/gemma-7b - will assume that the vocabulary was not modified.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67fe9b00-2917698b3da0a3954f888707;e7374bd7-0e68-44db-a754-f20cdd4e86e2)\n","\n","Cannot access gated repo for url https://huggingface.co/google/gemma-7b/resolve/main/config.json.\n","Access to model google/gemma-7b is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-7b.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in google/gemma-7b - will assume that the vocabulary was not modified.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67fe9b02-645e99e46f34a73f7292d0d3;aa61877b-53d7-4369-88a5-7854fefdb910)\n","\n","Cannot access gated repo for url https://huggingface.co/google/gemma-7b/resolve/main/config.json.\n","Access to model google/gemma-7b is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in google/gemma-7b.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in google/gemma-7b - will assume that the vocabulary was not modified.\n","  warnings.warn(\n"]}],"source":["\n","from transformers import TrainingArguments\n","# Step 4: Training setup\n","training_args = TrainingArguments(\n","    output_dir=\"./qlora_gemma_gsm8k_reasoning\",\n","    per_device_train_batch_size=4,\n","    gradient_accumulation_steps=8,\n","    num_train_epochs=3,\n","    learning_rate=2e-5,\n","    fp16=True,\n","    logging_steps=10,\n","    save_steps=200,\n","    save_total_limit=2,\n","    # evaluation_strategy=\"no\",\n","    eval_strategy='no',\n","    report_to=\"none\"\n",")\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer\n",")\n","\n","# Step 5: Train\n","trainer.train()\n","\n","# Save PEFT adapter\n","model.save_pretrained(\"./qlora_gemma_gsm8k/adapter\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.7"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1dca407a31f94b53b8c5518ebc24da58":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_92e5d87243a34497a8900f080edbedae","IPY_MODEL_f3dd4e6f6c7c43aea5607563c02a8729","IPY_MODEL_7b9e9e5da2b549248b606761400faa7c"],"layout":"IPY_MODEL_12b24569aff1446dbf4584cc7f5dd2ee"}},"92e5d87243a34497a8900f080edbedae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_080448f4f4db4f69b44e5b7a73f751a2","placeholder":"​","style":"IPY_MODEL_8ee7ffdfbfba4076b67a479ab5614da0","value":"Map: 100%"}},"f3dd4e6f6c7c43aea5607563c02a8729":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5bb4177686074528b58854cce7db98de","max":7473,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3e93eda585754a89b42c00927578c29c","value":7473}},"7b9e9e5da2b549248b606761400faa7c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_616dc3230faf4959a75db4ce09284475","placeholder":"​","style":"IPY_MODEL_7cb259562dab41d492f2605ebe722b34","value":" 7473/7473 [00:05&lt;00:00, 1275.18 examples/s]"}},"12b24569aff1446dbf4584cc7f5dd2ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"080448f4f4db4f69b44e5b7a73f751a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ee7ffdfbfba4076b67a479ab5614da0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5bb4177686074528b58854cce7db98de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e93eda585754a89b42c00927578c29c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"616dc3230faf4959a75db4ce09284475":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cb259562dab41d492f2605ebe722b34":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9afe9e51249e41c7a311ef8d325380fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4162d6278a3f48428b613d83651a4e56","IPY_MODEL_6ec521561cc343eeb40cade33be23c02","IPY_MODEL_68a0d27da022430b8885c031c228f13f"],"layout":"IPY_MODEL_6385e3b2293b4646803f17d2469f881d"}},"4162d6278a3f48428b613d83651a4e56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f85010846ad4a83aa6be214934789b0","placeholder":"​","style":"IPY_MODEL_9bc11577a21a474197804cd65223439c","value":"Loading checkpoint shards: 100%"}},"6ec521561cc343eeb40cade33be23c02":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ffe960af13842d08f124a5cf43db68e","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5957a9a92eb64bd79a02bf95f5946ddc","value":4}},"68a0d27da022430b8885c031c228f13f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7c44427d5c2444881c5a8424a49501a","placeholder":"​","style":"IPY_MODEL_ab8166aeb603412e8702c29b38545629","value":" 4/4 [00:20&lt;00:00,  4.71s/it]"}},"6385e3b2293b4646803f17d2469f881d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f85010846ad4a83aa6be214934789b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bc11577a21a474197804cd65223439c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ffe960af13842d08f124a5cf43db68e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5957a9a92eb64bd79a02bf95f5946ddc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c7c44427d5c2444881c5a8424a49501a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab8166aeb603412e8702c29b38545629":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}