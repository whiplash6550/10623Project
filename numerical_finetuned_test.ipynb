{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YQLTZapumPbk"},"outputs":[],"source":["import json\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","import re\n","from tqdm import tqdm\n","from peft import PeftModel\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/gemma_math_project/10623Project"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"SVdrPhSlmPbl","executionInfo":{"status":"error","timestamp":1744857204343,"user_tz":240,"elapsed":90,"user":{"displayName":"Xinglan Xu","userId":"00663053758062826624"}},"outputId":"2c1cffc7-41ec-467a-b1cb-a4a7290518f6","colab":{"base_uri":"https://localhost:8080/","height":211}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'AutoTokenizer' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-a06e038c4216>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 3) load tokenizer + base model locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"google/gemma-7b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m base_model = AutoModelForCausalLM.from_pretrained(model_name,\n","\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"]}],"source":["\n","# Load the model and tokenizer\n","#smodel_name = \"google/gemma-7b\"  # Instruction-tuned Gemmas\n","\n","\n","# 2) point to your model root and adapter folder\n","#base_model_path = \"/content/drive/MyDrive/models/qlora_gemma_gsm8k_reasoning/checkpoint-699\"\n","adapter_path    = \"/content/drive/MyDrive/gemma_math_project/10623Project/qlora_gemma_gsm8k/adapter\"\n","\n","\n","# 3) load tokenizer + base model locally\n","model_name = \"google/gemma-7b\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n","tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n","base_model = AutoModelForCausalLM.from_pretrained(model_name,\n","                                             torch_dtype=torch.bfloat16,\n","                                             device_map=\"cuda:0\",\n","                                             token=hf_token)\n","\n","# 4) overlay your locally‑saved LoRA adapter\n","model = PeftModel.from_pretrained(\n","    base_model,\n","    adapter_path,\n","    is_trainable=False,       # inference only\n","    local_files_only=True     # only look locally for the adapter\n",")\n","\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YF-B1V17mPbm"},"outputs":[],"source":["# Load test data\n","with open(\"gsm8k_reasoning_test.jsonl\", \"r\") as f:  # change to your actual file path\n","    test_data = [json.loads(line) for line in f]\n","\n","# Step 1: Extract the first answer block (robust to repeated Q/A)\n","def extract_first_answer_block(text):\n","    parts = text.split(\"A:\", 1)\n","    if len(parts) < 2:\n","        return text.strip()\n","    answer_section = parts[1].strip()\n","    stop_index = answer_section.find(\"Q:\")\n","    if stop_index != -1:\n","        answer_section = answer_section[:stop_index].strip()\n","    return answer_section\n","\n","# Step 2: Extract final answer\n","def extract_final_answer(text):\n","    match = re.search(r\"####\\s*([-\\d.,]+)\", text)\n","    if match:\n","        return match.group(1).replace(\",\", \"\")\n","    numbers = re.findall(r\"[-+]?\\d*\\.?\\d+\", text)\n","    return numbers[-1] if numbers else None\n","\n","# Step 3: Normalize answers\n","def normalize_answer(ans):\n","    if ans is None:\n","        return None\n","    ans = ans.replace(\",\", \"\").replace(\"$\", \"\").strip()\n","    try:\n","        return str(int(float(ans)))\n","    except:\n","        return ans\n","\n","# Inference in batches\n","batch_size = 16\n","results = []\n","\n","for i in tqdm(range(0, len(test_data), batch_size)):\n","# for i in tqdm(range(0, 4, batch_size)):\n","    batch = test_data[i:i+batch_size]\n","    # prompts = []\n","    # for item in batch:\n","    #     question = item[\"prompt\"].replace(\"Q:\", \"\").replace(\"A:\", \"\").strip()\n","    #     new_prompt = f\"Q: {question}\\n\\nWrite the final answer like this: #### [answer].\\n\\nA:\"\n","    #     prompts.append(new_prompt)\n","\n","    prompts = [item[\"prompt\"] for item in batch]\n","    # print(\"Prompts: \", prompts)\n","    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n","\n","    with torch.no_grad():\n","        outputs = model.generate(\n","            **inputs,\n","            max_new_tokens=256,\n","            do_sample=False,\n","            temperature=0.0,\n","            pad_token_id=tokenizer.eos_token_id\n","        )\n","\n","    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","    for j, output in enumerate(decoded):\n","        prediction = extract_first_answer_block(output)\n","        pred_ans = extract_final_answer(prediction)\n","        true_ans = extract_final_answer(batch[j][\"completion\"])\n","        correct = normalize_answer(pred_ans) == normalize_answer(true_ans)\n","        # print(f\"Question: {prompts[j]}\")\n","        # print(f\"Output: {output}\")\n","        # print(f\"Prediction: {prediction}\")\n","        # print(f\"True Answer: {true_ans}\")\n","        # print(f\"Predicted Answer: {pred_ans}\")\n","\n","        results.append({\n","            \"question\": batch[j][\"prompt\"],\n","            \"prediction\": prediction,\n","            \"pred_ans\": pred_ans,\n","            \"true_ans\": true_ans,\n","            \"correct\": correct\n","        })\n","\n","# Accuracy report\n","accuracy = sum(r[\"correct\"] for r in results) / len(results)\n","print(f\"\\n✅ Baseline Accuracy: {accuracy:.2%}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.7"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}